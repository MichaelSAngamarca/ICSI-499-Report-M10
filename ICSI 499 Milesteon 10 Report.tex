\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{tikz}

\begin{document}

% ----------------- TITLE PAGE -----------------
\begin{titlepage}
    \thispagestyle{empty}
    \centering
    \vspace*{2cm}

    {\large ICSI 499 Capstone Project Report \par}
    \vspace{1.5cm}

    {\Huge \textbf{TalkAssist} \par}
    \vspace{2cm}

    {\large \textit{Team 5} \par}
    \vspace{0.5cm}

    College of Nanotechnology, Science, and Engineering \\
    University at Albany, SUNY

    \vspace{3cm}

    {\large \textit{Project Sponsor:} \par}
    \vspace{0.5cm}

    Dr. Pradeep Atrey \\
    Athulya Mathew \\
    Department of Computer Science \\
    College of Nanotechnology, Science, Engineering \\
    University at Albany \\
    UAB 421, 1215 Western Avenue, Albany, NY 12222

    \vspace{3cm}

    {\large \today}
\end{titlepage}

% ----------------- FRONT MATTER -----------------
\pagenumbering{roman}

% ----------------- ACKNOWLEDGMENTS -----------------
\newpage
\thispagestyle{plain}
\begin{center}
    \Large \textbf{Acknowledgments}
\end{center}

\vspace{1cm}

\noindent
Team 5 would like to extend our heartfelt appreciation to Professor Pradeep Atrey and Athuyla Matthew for their unwavering support and encouragement. Their guidance throughout the development of TalkAssist inspired us, challenged us, and enabled us to transform our vision into a working product.

\vspace{0.5cm}

\noindent
The motivation behind Talk Assist was to develop a tool that people with life-altering visual disabilities might utilize to live their lives on an equal footing with people without them. We aim to sustain this purpose and produce a product that satisfies these great expectations placed upon us by our sponsors and the University at Albany.

% ----------------- ABSTRACT -----------------
\newpage
\thispagestyle{plain}
\begin{center}
    \Large \textbf{Abstract}
\end{center}

\vspace{1cm}

\noindent
Talk Assist is an artificial intelligent voice assistant designed for individuals with visual disabilities. The project's objective is to create, test, and assess an AI-driven voice assistant that supports visually impaired users in managing daily life. Talk Assist includes two unique modes that allow users to dynamically transition based on Internet connectivity. Users can request online searches, weather, time, date information, and reminders in Online Mode. In Offline Mode, Talk Assist preserves the same weather, time, date, and reminder functionality, while also offering a calculation utility for basic computations. Talk Assist aims to simplify daily tasks for visually impaired users without the need for expensive or inaccessible healthcare technologies.

% ----------------- PROBLEM ANALYSIS -----------------
\newpage
\pagenumbering{arabic}

\section{Problem Analysis}

\subsection{Project Statement}
Individuals with visual impairments often face significant challenges when interacting with everyday technology. Tasks such as reading screens, managing schedules, accessing online information, or performing basic device operations can require specialized tools or expensive accessibility technologies. Many existing solutions lack adaptability, rely heavily on constant internet connectivity, or are too complex for seamless daily use.

TalkAssist aims to address these challenges by providing an intuitive, AI powered voice assistant specifically designed for users with limited or no vision. The goal of this project is to design, implement, and evaluate a system that supports both online and offline functionality, delivers accurate voice feedback, and simplifies daily tasks through natural speech interaction. By integrating reliable recognition, high quality text-to-speech components, and an interface tailored for accessibility, TalkAssist seeks to make digital interaction more equitable and practical for visually impaired individuals.

\subsection{What are Existing Solutions?}
Several solutions exist in the accessibility and voice-assistant space, such as Apple’s VoiceOver, Google Assistant, and Amazon Alexa. While powerful, these tools are developed for broad consumer audiences rather than tailored explicitly for visually impaired users. As a result, they often require complex setups, consistent cloud connectivity, or rely on visual interfaces not optimized for non-sighted users.

Some standalone accessibility devices provide improved usability such as scre, but they can be costly and inaccessible to many. These systems also frequently depend on external hardware or lack offline capability, making them unreliable in environments with unstable internet connections.

Additionally, visually impaired users report issues such as inconsistent voice feedback, poor environmental adaptability, and difficulty performing basic tasks when offline. Users often struggle to transition smoothly between connected and disconnected settings, resulting in a fragmented user experience.

\subsection{Our Solution}
TalkAssist provides a unified, accessibility focused platform designed to meet the needs of visually impaired users in both online and offline contexts. Our system introduces the following key features:

\begin{itemize}
    \item \textbf{Dual-Mode Functionality:} Users can operate TalkAssist in Online Mode for tasks such as web searches, weather updates, and general questions, while Offline Mode preserves essential functions like time, date, reminders, and basic calculations.
    
    \item \textbf{Accessible Voice Interaction:} The system is designed around natural speech input and clear spoken output, minimizing dependence on visual cues or screen navigation.
    
    \item \textbf{Task Management and Reminders:} Users can create, update, and manage reminders through simple voice commands in both online and offline modes, supporting daily independence.
    
    \item \textbf{Lightweight and Low-Cost Design:} Unlike expensive commercial assistive devices, TalkAssist runs on standard consumer hardware, making it affordable and widely accessible.
\end{itemize}

Together, these features create a streamlined accessibility tool that empowers visually impaired individuals to manage tasks, retrieve information, and interact with technology more independently and confidently.

\subsection{Overview of Report}
The remainder of this report is structured as follows:

\begin{itemize}
    \item \textbf{Section 2} describes TalkAssist's design and capabilities, including its API architecture, applications, and implementation techniques.
    \item \textbf{Section 3} describes the testing procedure, system performance analysis, and research design.
    \item \textbf{Section 4} addresses security, legal, and ethical issues like user privacy, data security, and dependency on third-party APIs.
    \item \textbf{Section 5} presents a breakdown of the roles and contributions of the team.
    \item \textbf{Section 6} highlights potential future enhancements and growth opportunities for Talk Assist before concluding the paper.
\end{itemize}

%SECTION 2 

\section{Proposed System/Application/Study}

\subsection{Overview}

TalkAssist is designed to support individuals across a broad spectrum of visual impairments. To effectively serve this user group, the system is built around three core requirements: a fully non-visual interface, highly accurate text-to-speech and natural language processing capabilities, and complete hands-free operation through voice interaction. In this section, we provide an overview of our project requirements, describe our system architecture, and walk through the major components of our implementation. We also highlight the core computer science and software engineering principles applied throughout the project including time-complexity considerations, natural language processing techniques, API integration, and system integration techniques.

\subsection{Project Requirements}

\subsubsection{User Classes}
There are two distinct user classes. Application Users are individuals who interact directly with TalkAssist. This group includes users with visual impairments who rely on fully voice-driven, non visual interaction. Their primary goal is to communicate with the chatbot, receive accurate spoken responses, and operate the system entirely hands-free. Event Management Users rely on TalkAssist to manage time based tasks such as reminders, alerts, and scheduled notifications. To stay organized without requiring visual involvement, these users rely on precise voice-controlled scheduling and clear audio confirmations.

\subsubsection{Functional Requirements}
\begin{itemize}
\item\textbf{Front End GUI:} Talk Assist requires a simple and clear front-end GUI that is able to transcribe conversations and also indicate when either offline or online mode is currently running. The GUI must also contain a reminders page that clearly shows each active reminder and is able to differentiate between active and inactive reminders.

\item\textbf{Online Mode:} Talk Assist requires an online mode that is able to perform commands such as telling the time, weather, date, and setting reminders. It should also be able to search the web for answers to questions asked by the user. We require this online connection as Talk Assist’s online mode has many more capabilities and is meant for at-home use.

\item\textbf{Offline Mode:} Talk Assist requires a lightweight offline mode that is able to perform similar but more basic tasks than its online mode. Offline mode should still be able to tell time, weather, date, and set reminders. There are no web search capabilities, but this is offset by a basic calculation tool that allows offline mode to perform operations requested by the user. Offline mode is lightweight due to this mode being intended for out-of-home use, such as while walking or grocery shopping.

\item\textbf{Wake Up Word \& Hot Key:} Due to Talk Assist’s hands-off approach to operation, we require wake-up word and hotkey capabilities. The wake-up word is meant to activate the program and start a conversation once detected, allowing the user to avoid restarting the program to begin a new conversation. The hotkey is required to allow the user to launch the application and begin microphone recording for wake-up word activation, enabling them to avoid navigating a screen and streamlining the application launch process.
\end{itemize}

\subsubsection{Non-Functional Requirements}
\begin{itemize}
\item\textbf{Accessibility:}The system must be fully usable without visual navigation (screen-reader friendly, voice-first interaction)
\item\textbf{Usability:}The chatbot must have a simple, natural, and intuitive voice-based interaction flow. Responses should be clear, concise, and easy to understand for users with varying levels of technical skill
\item\textbf{Scalability:}App should support integration with additional features ( calendars, weather APIs, reminders) without major redesign
\item\textbf{Portability:}Should scale to run on multiple platforms in future (desktop and mobile) with minimal configuration
\end{itemize}

%Section 3

\section\textbf{Experimental Design and Testing}

This section presents the experimental methodology used to evaluate TalkAssist across its online mode, offline mode, voice-driven functionality, and graphical user interface (GUI) components. The goal of these experiments is twofold: \textbf{verification} – confirming that TalkAssist’s implemented features behave correctly – and \textbf{validation} – determining whether the system adequately addresses the needs of visually impaired users who rely on fully voice-operated systems. Tests were conducted by multiple team members, each focusing on different subsystems: online speech recognition reliability, offline speech-to-text and reminder management, mathematical query handling, and frontend accessibility. Collectively, these experiments provided a comprehensive assessment of TalkAssist’s usability, accuracy, and robustness in realistic user conditions.

\subsection\textbf{3.1 Experimental Setup}

We designed five experiments, numbered by subsystem:

\begin{enumerate}
    \item Online Voice-Only Navigation Test
    \item Online Speech Recognition Across Recording Environments
    \item Accuracy of Mathematical Query Parsing
    \item Offline Speech Recognition and Offline Reminder Management
    \item Frontend Recording-Indicator Behavior (GUI Test)
\end{enumerate}

Each experiment was conducted to target different risk areas, including voice accuracy, robustness under noise, correct command interpretation, deterministic behavior offline, and the accessibility of the user interface for non-visual users. Because TalkAssist is intended for visually impaired users, tests emphasized hands-free usage, speech-only interaction, and clarity of system feedback.

\subsubsection\textbf{3.1.1 Experiment 1: Online Voice-Only Navigation Test}

\textbf{Objective:} To determine whether TalkAssist’s online version can be fully navigated without visual access to the GUI.

\textbf{Setup:} The tester shut their eyes or used a pillowcase to simulate blindness. The online version of TalkAssist ran on a Windows laptop in a quiet room. Only voice commands were used to navigate between features such as time queries, reminders, and basic Q\&A responses.

\textbf{Environment \& Tools:}
\begin{itemize}
    \item Chrome browser running TalkAssist web interface
    \item Built-in laptop microphone
    \item No keyboard or mouse input permitted
\end{itemize}

\textbf{Purpose:} To validate whether TalkAssist meets its accessibility goal: functioning as a screen-reader-free, voice-only assistant.

\subsubsection\textbf{3.1.2 Experiment 2: Online Speech Recognition Across Varying Recording Environments}

\textbf{Objective:} To test the reliability of online speech recognition under changes in microphone quality and ambient noise.

\textbf{Setup:} Several recording environments were tested:

\begin{itemize}
    \item Quiet bedroom
    \item Medium-noise room (TV and conversation noise)
    \item Crowded living room
\end{itemize}

Additionally, multiple microphones were used (laptop mic vs. USB headset).

\textbf{Purpose:} To determine if TalkAssist maintains voice recognition accuracy across common, real-world usage environments, as many visually impaired users rely on built-in laptop hardware that varies widely in quality.

\subsubsection\textbf{3.1.3 Experiment 3: Mathematical Query Parsing}

\textbf{Objective:} To test the chatbot’s ability to correctly handle mathematical computations using natural language.

\textbf{Setup:} A series of spoken math prompts were used, including:

\begin{itemize}
    \item Trigger-word-based commands: “What is 5 plus 7?”
    \item Multi-phrase queries: “Calculate the sum of 5 and 4.”
    \item Ambiguous phrases involving “times” to ensure the bot distinguishes multiplication from time queries.
\end{itemize}

\textbf{Purpose:} To validate the correctness of the math parser and ensure disambiguation logic prevents false triggers.

\subsubsection\textbf{3.1.4 Experiment 4: Offline Speech Recognition \& Reminder Management}

\textbf{Objective:} To evaluate TalkAssist’s offline functionality, which is critical for accessibility in situations where internet access is unavailable.

\textbf{Setup:}

\textbf{Hardware:} Windows 11 laptop, USB microphone

\textbf{Software:} Whisper base model (70M parameters) running locally, Python 3.10 virtual environment

\textbf{Environments:}
\begin{itemize}
    \item Quiet room
    \item Medium background noise (open window, distant voices)
\end{itemize}

Offline module tests included:

\begin{itemize}
    \item Speech-to-text accuracy
    \item Local reminder creation, storage, and retrieval using \texttt{reminders.json}
    \item Offline text-to-speech playback
\end{itemize}

\textbf{Purpose:} To ensure the assistant remains usable in scenarios with poor connectivity: highly relevant to low-vision users in unpredictable environments.

\subsubsection\textbf{3.1.5 Experiment 5: GUI Recording-Indicator Behavior}

\textbf{Objective:} To test whether the TalkAssist interface correctly indicates when the system is actively listening for user input. This behavior is important for accessibility because visually impaired users rely on consistent feedback to understand when it is their turn to speak.

\textbf{Setup:} The tester interacted with TalkAssist using standard voice commands and manually observed the single available frontend indicator:

\begin{itemize}
    \item The recording indicator, which is designed to appear only when user audio input is being captured.
\end{itemize}

\textbf{Purpose:} To determine whether the recording indicator switches states properly during typical voice interactions and whether it provides reliable feedback to users who depend on nonvisual cues.

\textbf{Known issue uncovered:} The bot continues listening during its own speech, causing the recording indicator to remain active incorrectly. This experiment helped isolate whether the issue originated from frontend logic, asynchronous Web Speech API behavior, or backend latency.

\subsection\textbf{3.2 Dataset}

Because TalkAssist relies heavily on natural language input, each experiment generated a dataset of spoken commands and system outputs. The datasets are described below per experiment.

\subsubsection*\textbf{Dataset for Experiments 1 \& 2 (Online Mode)}

\begin{itemize}
    \item 2 internal testers simulating visually impaired usage
    \item $\sim$60 navigation and Q\&A voice commands
    \item 40 environmental speech samples across three noise levels
    \item Microphone variability tests across 2 devices
\end{itemize}

Data included:
\begin{itemize}
    \item STT transcriptions
    \item Intent parser outputs
    \item Success/failure flags for navigation tasks
\end{itemize}

\subsubsection*\textbf{Dataset for Experiment 3 (Math Parsing)}

\begin{itemize}
    \item 25 spoken mathematical expressions
    \item Mix of operator-based and phrase-based queries
    \item Misinterpretation cases logged for debugging
\end{itemize}

\subsubsection*\textbf{Dataset for Experiment 4 (Offline Mode)}

\begin{itemize}
    \item 110 spoken test phrases
    \item Whisper accuracy logs
    \item \texttt{reminders.json} operations (creation, deletion, listing)
\end{itemize}

\subsubsection*\textbf{Dataset for Experiment 5 (GUI Test)}

Collected via frontend logging:

\begin{itemize}
    \item 30 turn-taking sequences
    \item 30 observation logs of indicator behavior
    \item Timestamps of listening vs. speaking phases
\end{itemize}

\subsection\textbf{3.3 Results and Analysis}

\subsubsection\textbf{3.3.1 Experiment 1 Results: Online Voice-Only Navigation}

Users were able to navigate most features without visual assistance. Event-level tasks (time queries, general questions, reminder queries) succeeded consistently.

\textbf{Failure point:} accessing the reminders page required too strict phrasing and occasionally misrouted the user.

This indicates a need for broader intent recognition or additional aliases for “open reminders.”

\subsubsection\textbf{3.3.2 Experiment 2 Results: Voice Recognition Across Environments}

TalkAssist functioned reliably in quiet and medium-noise environments. When used with a poor-quality laptop microphone:

\begin{itemize}
    \item Misrecognitions increased
    \item Reminder texts occasionally stored incorrect words
    \item Long commands were more error-prone
\end{itemize}

This matches expected patterns in consumer STT systems: microphone quality is the primary determinant of accuracy.

\subsubsection\textbf{3.3.3 Experiment 3 Results: Mathematical Query Parsing}

All math expressions produced correct results. The implemented rule to disambiguate:

\begin{itemize}
    \item “times = multiplication”
    \item “time = current time request”
\end{itemize}

worked as intended, preventing false time-query activations.

This confirms the math parser’s dependability and robustness to ambiguous phrasing.

\subsubsection\textbf{3.3.4 Experiment 4 Results: Offline Whisper STT \& Reminder System}

\textbf{Whisper Offline Accuracy}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Test Phrases} & \textbf{Correct} & \textbf{Accuracy} \\
\hline
110 & 88 & 80\% \\
\hline
\end{tabular}
\caption{Offline Whisper STT accuracy results}
\end{table}

Whisper performed slightly worse than online STT but remained usable for reminders and date/time queries. Whisper’s deterministic behavior significantly improved reliability for structured tasks such as reminders.

\textbf{Reminder System}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Task} & \textbf{Success Rate} \\
\hline
Set Reminder & 100\% \\
List Reminders & 100\% \\
\hline
\end{tabular}
\caption{Offline reminder system performance}
\end{table}

Offline mode proved highly stable because it avoids ambiguity and latency associated with large online models.

\subsubsection\textbf{3.3.5 Experiment 5 Results: GUI Indicator Test}

Results were summarized in the following table:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Test Case} & \textbf{Expected Behavior} & \textbf{Observed Behavior} & \textbf{Success} \\
\hline
Bot Speaking & Indicator OFF & Indicator remained ON & No \\
Bot finished speaking & Indicator ON & Correct & Yes \\
Rapid turn-taking & Indicator alternated correctly & Sometimes lagged & Somewhat \\
Long bot responses & Indicator OFF & Indicator ON entire duration & No \\
\hline
\end{tabular}
\caption{GUI indicator test results}
\end{table}

Analysis: The issue stems from the online SpeechRecognition API continuously streaming even during bot speech. Because TalkAssist is voice-driven, this causes confusion for a visually impaired user who cannot rely on visual timing cues.

\subsubsection\textbf{3.3.6 Expected and Unexpected Findings}

\textbf{Expected:}
\begin{itemize}
    \item Whisper offline performs slightly worse than online STT
    \item Math parser reliably handles structured phrases
    \item Online navigation works well in quiet environments
\end{itemize}

\textbf{Unexpected:}
\begin{itemize}
    \item Offline reminders were more reliable than online reminders
    \item GUI indicator issue was persistent across all tests
    \item Microphone quality had a greater impact than anticipated
\end{itemize}

\subsubsection\textbf{3.3.7 Remarkable Findings}

The most notable positive finding was the robustness of offline reminder functionality.

Unlike online mode, offline mode has:

\begin{itemize}
    \item No LLM-based ambiguity
    \item Deterministic parsing
    \item Zero network latency
\end{itemize}

As a result, offline mode became the most stable subsystem, which is valuable for visually impaired users who may not always have reliable connectivity.

\subsubsection\textbf{3.3.8 Failure Cases \& Limitations}

Key limitations identified include:

\begin{itemize}
    \item Recording indicator remains active during bot speech
    \item Lower accuracy in noisy environments or with poor microphones
    \item Navigation to specific pages occasionally requires precise phrasing
    \item Offline Whisper may run slowly on low-end hardware
\end{itemize}

These findings will guide future improvements, including better frontend event handling, expanded intent recognition, and optimization of the offline pipeline.

%Section 4

\section*{4 \quad Legal and Ethical Practices}
\addcontentsline{toc}{section}{4 Legal and Ethical Practices}

\subsection*{4.1 \quad Legal Considerations}
\addcontentsline{toc}{subsection}{4.1 Legal Considerations}

\textbf{Use of Third-Party Libraries:}
Only speech-to-text, text-to-speech, and API tools with proper open-source or permissive licenses (MIT, Apache, etc.) were used to avoid copyright or IP violations.

\textbf{Reminder Data Storage:}
\begin{itemize}
    \item TalkAssist does not store voice recordings.
    \item It does save reminder text (e.g., ``Take medication at 3 PM'') in a local JSON file.
    \item Because this information may be personal or sensitive, all stored data remains on the user's device.
\end{itemize}

\textbf{Privacy and Data Protection:}
Local-only storage reduces risks related to major data-protection laws (GDPR, state privacy laws). Users are informed that reminder text is stored to ensure transparency.

\textbf{Online Service Compliance:}
When TalkAssist uses online features (e.g., weather, general information), the system abides by the terms and usage limits of each external API.

\textbf{Liability Reduction:}
Clear limitations are placed on high-risk tasks, and the assistant avoids giving medical, legal, or dangerous instructions.

\vspace{1em}

\subsection*{4.2 \quad Ethical Considerations}
\addcontentsline{toc}{subsection}{4.2 Ethical Considerations}

\textbf{Protection of Vulnerable Users:}
Designed for individuals with visual impairments, the system avoids misleading, unsafe, or unclear responses.

\textbf{Privacy \& User Consent:}
\begin{itemize}
    \item Reminders may contain private routines or sensitive tasks.
    \item Users are informed that reminders are stored locally and may delete them at any time.
    \item No hidden data collection or cloud storage is used.
\end{itemize}

\textbf{Data Minimization:}
Only essential reminder information is saved—nothing more.

\textbf{User Autonomy \& Control:}
Users can freely create, edit, or remove reminders; the assistant never makes decisions without explicit instruction.

\textbf{Transparency of Modes:}
The bot clearly distinguishes between offline processing (fully private) and online processing (which may contact external APIs).

\textbf{Inclusivity and Accessibility:}
Every feature is designed to enhance independence, clarity, and accessibility for visually impaired users.

%Section 5
\section*{5 \quad Effort Sharing}

\begin{table}[h!]
\centering
\caption{Effort sharing}
\scriptsize                     % smaller font for this table
\setlength{\tabcolsep}{3pt}     % reduce horizontal padding
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Team size} &
\textbf{Joint efforts} &
\textbf{Michael Angamarca} &
\textbf{Isaac Jean Gardy Mardy} &
\textbf{Ayan Cooper} &
\textbf{Kyle Batchelor} \\
\hline
4 & J (30\%) & I (17.5\%) & I (17.5\%) & I (17.5\%) & I (17.5\%) \\
\hline
\end{tabular}

\vspace{2mm}
\footnotesize
J = description of tasks jointly performed \\
I = description of tasks individually performed
\end{table}

%Section 6
\section*{6 \quad Conclusion and Future Work}

The development of TalkAssist allowed us to design and implement an accessible
voice driven assistant that supports both online and offline functionality for
visually impaired users. Through this project, we learned how to integrate
speech technologies, manage system modes, and design intuitive interactions that
reduce dependence on visual interfaces. While the current system performs core
tasks reliably, future work includes expanding offline capabilities, improving
natural language understanding, and integrating additional online services such
as calendars and navigation. Long term improvements may also include mobile
platform support and a more advanced wake word system to further enhance
accessibility and user independence.

\end{document}

